{
  
    
        "post0": {
            "title": "Effective and Ethical Data Science Study Design",
            "content": "Machine learning is the process of modelling our data, creating a smaller-scale representation shaped to highlight the targeted aspects of it. Just as a sculptor works their medium to create statues highlighting valour, beauty or anatomical correctness, training data is the medium you shape into your model to represent your goals. If you have a low-quality medium, you’ll have a low-quality model no matter how talented the craftsperson is. Data sampling is the process used to generate a high-quality medium from which to craft a machine-learning model. . Machine Learning is used for both art and science. When used for art, subjective, evocative, biased portrayals are the goal. When used for science, objective, clinical, unbiased models are the goal. To create an objective model, a universally representative training dataset is required. In practice, we do our best to match the “global distribution” of our target problem. . In applied Data Science, we are responsible for selecting our data. To use machine learning in a scientific setting, how can you effectively and ethically aim for this data to match the global distribution? . Statistical Studies . Statistical studies explore statistical questions. A statistical question is a question that requires us to collect data with variability to provide an answer. An example of a statistical question is “what is the average age of the employees in this company?” which requires you to collect the ages of all the employees in the company, with their variance. Asking, “what is the average age of all children in this class?” can be a non-statistical question based on context. If you have restricted the class to only run for a single age, then there is zero variance. The question “what age are you?” is a non-statistical question since it has a single answer with zero variability. . Your question will have a contextual target population, and you are unlikely to be trying to find some genuinely universal insight (at least to start with). Instead, you may focus on a lab, an organisation, a country, existing customers of your product or a new target demographic. . Your target population will often be too large to collect feature values for every element in the population. In this case, a population subset is selected using data sampling methods. There are a variety of statistical study methodologies to choose from, whether using the whole target population or a sampled subset. . Sample Study . In a sample study, you investigate the prevalence of a single feature within your target population. You can’t say anything more about this feature other than its prevalence and you gain no insight into correlation or causation. It is simply a data collection study to provide you with information to take additional future actions. . If your sample is your entire target population, then you have your answer! The exact prevalence of this feature within your target population. This value does not tell you anything about the prevalence of this feature outside your target population. If you have sampled a subset of your population, you extrapolate from your sample to estimate the prevalence in the total target population. . Executing sample studies enables you to discover candidate features for an observational study for this specific target population. . Observational Study . In an observational study, you investigate how two features correlate within your target population. . You can only say their degree of correlation; you cannot claim causation. If X and Y correlate, we have no information regarding whether X causes Y, Y causes X, or if an unexamined confounding feature Z causes both or even some more complicated relationship. . If your sample is your entire target population, then, again, you have your answer. The exact degree to which these features correlate within your target population, but this answer does not tell you the degree to which they correlate outside your target population. If you have sampled a subset of your population, then you extrapolate from your sample to estimate the prevalence in the total target population, which will have some degree of error. . Carrying out observational studies enables you to discover candidate features for an experimental study for this specific target population. . Experiment . In an experiment, you investigate the causality between two features within your target population. Each element in your sample gets assigned to a control or treatment group, and you vary an “independent” feature between groups and observe any changes in a target “dependent” feature. . There are many additional challenges to tackle in experimental design to determine causality reliably. . Experiments need to be careful to avoid placebo effects | Experiments need to be careful to avoid information leakage. One technique to address this is double-blind studies | Experiments must fairly assign elements to control and treatment groups, avoiding issues with confounding variables | . Fairly assigning elements to control or treatment has similar challenges to the initial data sampling. Some experiments will do a random assignment, similar to random sampling, and others will try to control for confounding variables via block design, similar to stratified sampling. . With appropriate sampling, assignment to groups and general experiment design, you can make a strong claim about the degree of causality between features within the target population. . What Study Types Can I Do With Machine Learning? . Machine Learning derives from the practice of observational study. Machine Learning models are trained to detect correlates and use these for prediction. Machine Learning models are not trained as experimental studies; they do not show causality. Machine Learning models use discovered correlations to make inferences, regardless of causality. If you control for confounding variables, you may find no correlation between these features, which shows there is no causal relationship. . Running a series of laborious experiments may be unfeasible, so often, we use observational studies as a proxy. The underlying assumption of applied machine learning is that the greater generalisation from some samples to other samples, the more likely the model is to operate on causal relationships. . Data Sampling . Many Data Science practitioners overlook that they apply inference long before they build an ML model. Data sampling is the process of applying statistical methods to select data samples from the domain. When sampling, we make a statistical inference about the population from our limited knowledge and our small set of observations. It is crucial we consider how we infer the global distribution of our problem domain during data collection. Any errors in this upstream inference will propagate down to the ML model’s ability to solve the actual target problem. . You cannot avoid data sampling; it will happen implicitly or explicitly. If done implicitly, you will be unable to assess the true effectiveness of your total solution or monitor any ethical implications of underperforming in specific demographics. . Data Sampling Methodology . The features and meta-features required to carry out a population distribution prediction are: . The scope of the population you wish to target | The property you want to estimate | The sample size you will gather | The sampling methodology you will use | Common sampling methodologies are: . Probability sampling Simple random sampling: drawn with a uniform probability from the domain | Systematic sampling: draw samples using a pattern, e.g. at intervals | Stratified sampling: draw samples from predefined categories | . | Non-probability sampling Quota sampling: the non-probability based equivalent of the stratified sampling | Voluntary sampling: volunteered samples are drawn from the target population | Convenience sampling: include the most accessible samples. Often the non-probability based equivalent of systematic sampling | Purposive sampling: samples are selected based on the judgement of the researcher. The credibility of any extrapolations to the target population is based on subjective interpretations of the researcher’s authority as an expert in their field and faith in their ability to make reasonable calls. | . | . When using non-probability sampling, it is impossible to determine the possible sampling errors and make reliable statistical inferences from the sample to the population. When applying sampling, the dataset can end up with a skewed data distribution due to errors such as: . Selection bias: skew caused by the method of selecting samples | Sampling error: skew caused by the randomness of sampling | Systematic errors in the practice of making observations and measurements Under-coverage: The researcher systematically excludes members of the population from being in the sample | Response bias: Systematic false responses, often due to emotional reactions to the problem definition or methodology | . | Voluntary response bias: there is a non-random pattern in which samples are volunteered | Non-response: there is a non-random pattern in which samples cannot be collected | . To assess biases across different features within your sample, you will assess the feature’s distribution of values. Its measurement scale determines its possible values: . Nominal: variables that are not numerical, e.g. categories like gender and ethnicity | Ordinal: variables where order has meaning, but the magnitude of difference between values is not important, e.g. ranks like 1st, 2nd and 3rd | Interval: variables where the magnitude of difference between values is important, e.g. actual numbers like the temperature in °C | Ratio: Interval data with a natural (absolute) zero point, e.g. Time in seconds, has a ratio scale, but temperature in °C does not (since 0°C does not mean no heat) | . Probability Sampling Techniques . All probabilistic sampling techniques require you to obtain a complete and up-to-date list of all members of your target population. Probability sampling techniques are a necessary basis for any experimental design. Only a quasi-experiment is possible if a non-probabilistic sampling technique is used. . Simple Random Sampling . A random selection is chosen from your total target population list, often by using a random number generator. This technique should be effective at creating representative samples across a large number of experiments, each with its own generated dataset. But it does not make any guarantees for your individual dataset. Your individual sample may have sampling errors, causing it not to represent the demographics of the global population, e.g. you may randomly select more males than females, even if both are equally represented in the population. . Systematic Sampling . Systematic sampling selects members from the entire target population list at regular intervals across some axis. If this sampling axis is genuinely random, then this technique is no different from random sampling. One improvement over random sampling is it naturally avoids clustered selection, where selected samples are unnaturally close together. To achieve this same effect with simple random sampling, the sample size would need to be increased, which can be expensive. . Like the other probabilistic sampling methods, systematic sampling requires the entire target population list to take intervals from. If you don’t have this list, you cannot determine the min and max of the axis along which you will sample to determine an appropriate interval. Taking samples at regular intervals, from a target population, without the complete list of the target population is a form of convenience sampling, which is non-probabilistic. . Systematic sampling is at high risk of selection bias. If there is a non-random pattern along the sampled axis, this could cause a skew in the sample distribution. E.g. If the HR database groups employees by team, and team members are listed in order of seniority, there is a risk that your interval might skip over people in junior roles, resulting in a sample that is skewed towards senior employees. . The chosen axis across which to sample may also be a confounding feature for the target of the study, making the study easily manipulated by consciously refining the interval to alter the study’s results. . Stratified Sampling . Stratified sampling divides the total list of the target population into subgroups representing specific features of the samples. Members are randomly sampled from within each of the subgroups per their ratio to the entire target population. This selection avoids potential bias from subgroups being randomly over-represented or under-represented. . Stratified sampling has several challenges. Some subgroups may be very small, and to include them in your sample with their appropriate ratio, you may need to take a larger sample from your target population. Stratified sampling assumes the subgroups are exhaustive and mutually exclusive. If a member appears in multiple subgroups, they are more likely to be chosen, which could result in a misrepresentation of the total target population. . Stratified sampling also requires you to decide upon the applicable sub-populations themselves. To split on continuous variables, you must select appropriate buckets. For discrete variables, you must decide which relevant categories exist in your study. This is a form of feature engineering, determining your sub-populations. . Like the other probabilistic sampling methods, stratified sampling requires the whole target population list. You need this list to determine the ratios of samples from each subgroup to make your full sample representative of the target population. Making assumptions about the subgroup distribution across the target population is a form of quota sampling, which is non-probabilistic. . Non-Probability Sampling Techniques . Non-probability sampling techniques are used when it is unfeasible to obtain a complete and up-to-date list of all members of your target population. Non-probability sampling techniques cannot be used for experimental design, as the study results cannot claim causality without randomness in sample selection. Only a quasi-experiment is possible if a non-probabilistic sampling technique is used. . Convenience Sampling . A sample is taken from the target population based on the members that are most accessible. No other inclusion criteria are applied. Convenience sampling is an easy and inexpensive way to gather data, but no claims can be made about the sampled data distribution to determine if any analysis could yield generalisable results. . Voluntary Response Sampling . The sample from the target population is the set of members who volunteer, which has the cost-benefit of removing the burden on the researcher to locate members themselves. Having members who actively volunteered can make gathering additional metadata much easier, as the person or organisation is likely eager to assist in the study. . Your sample is prone to voluntary response bias. Specific samples are more likely to volunteer than others, which could be due to some underlying feature relevant to the study. . Voluntary response sampling is the type of sampling that occurs when a researcher works with open-source datasets. . Purposive Sampling . The researcher chooses their sample based on criteria internal to their mind. This technique is especially prone to researcher bias. It can theoretically be effective if the subjective judgement of the researcher is based on well-considered clear criteria. Faith in this approach comes down to trust in the researcher’s authority. It can be difficult for a researcher to convince the reader of the representativeness of their sample. Ref . One purposive sampling strategy is [maximum variation sampling](https://www.statology.org/maximum-variation-sampling/#:~:text=Maximum%20variation%20sampling%20(sometimes%20referred,possible%20about%20a%20certain%20topic.), with the goal of collecting the broadest range of perspectives possible. The theory is that by sampling members that are extremely different to each other, the researcher can gain a more holistic overview. . Homogeneous sampling is another purposive sampling technique, which is opposite in theory to maximum variation sampling. The theory is that the researcher will yield greater insight by focusing on similar samples. . Quota Sampling . Quota sampling is very similar to stratified sampling. It has two stages, first, defining subgroups, and second, sampling from them. . The first stage can be identical for both quota and stratified sampling. You can determine the true ratio between subgroups if you have the complete target population list. Given this, whether or not you carry out quota or stratified sampling is based solely on the second step. If you don’t have the complete target population list, you must make assumptions about the subgroup distribution. These assumptions make your sampling process non-probabilistic, transforming it into quota sampling. . In the second stage, stratified sampling randomly samples from the subgroups while matching their distribution to that of the total population. Quota sampling can use the same technique but with an assumed population distribution. Alternatively, samples are taken from subgroups based on convenience or expert judgement. . Quota sampling is often used due to non-functional constraints. For example, a researcher may judge that it is ethically important to include a subgroup but may need more money to scale their sample to a size that maintains the population distribution between subgroups. They may determine that it is more ethical and effective to include a member from this subgroup in their study, even though this subgroup will now be over-represented relative to the population. . Ontology Engineering . When conducting stratified or quota sampling, a researcher must define their strata. I find it helpful to frame the problem of determining your appropriate strata as “ontology engineering”. Your ontology is the universal set of onta (or categories) considered for tackling your problem. What you define in your ontology has huge implications for the effectiveness and ethics of your solution. . If metadata is not collected for an ontum, it cannot be considered in your sampling. If it is not considered in your sampling, it cannot explicitly influence your population distribution prediction. If it cannot influence your population distribution prediction, you cannot detect and account for errors and biases concerning this ontum. . In Data Science contexts, class, feature, category, or term are words often used to refer to the concept of what I am here calling an ontum. The alternatives are highly overloaded and applied in many divergent contexts. Ontum has the additional benefit of having the plural onta and the syntactically related term ontology for describing the universal set for a given domain. . For the example of gender, your ontology may consider 2-58 gender identities. If you choose an ontology of size two, you have made it impossible to provide a solution considering the other gender identities. If you select an ontology of size 58, you have made it possible to misclassify anyone as any of these 58 onta. Even if your ontology defines 58 universal gender identities, you may be unable to sample and represent them all, as some onta may be present in very small proportions of the population. It will, however, be possible to identify and highlight this and make consumers of your model aware of its limitations. . When evaluating the success of your ML model, you will see systematic and random errors. Unaccounted for onta will often appear as “random errors”. They are called random while not actually being random at all. They have an underlying systemic cause, but their distribution cannot be accounted for because these onta don’t exist within your ontology. It is theoretically possible to account for all “random” errors (for deterministic models) by uncovering new onta. What is often dismissed as “random error” means errors caused by an “unknown unknown”. Some factor that exists outside of our monitored ontology is affecting the performance of our inference. . Determining your ontology is inherently philosophical and political for these reasons. Through inclusion or exclusion, you are deciding which “random” errors can be systematically present and remain unaccounted for. Defining your ontology based on the status quo is not apolitical but conservative. . Oversight of Data Sampling in the Data Science Community . Data Sampling is often overlooked due to a methodological bias in the Data Science community. Data Science is traditionally practised as experimenting with different modelling techniques and methodologies against a static set of benchmark datasets. This allows the ML modelling techniques to be evaluated in isolation from the data sampling. This can work well for academic research but does not translate well into industry, where Data Science is only of value when applied to bespoke new datasets created/curated by the organisation. . This process of iterating on a static set of benchmark datasets also inherently embeds biases into the researched techniques. This causes ethical and effectiveness issues when applied to new datasets. This is because these datasets are not “inclusive datasets”, and they have been created by a small subset of people with authority in the field and the power to promote them. These people have chosen what we focus on for research from within their biassed paradigm of what is normal, what is abnormal, what problems are worth considering and what is not. This is a result of both their nature and nurture. This means that these datasets often focus on specific demographics, languages, and cultural traits. In the problem solution space of ML model architecture and methodology, we will be trapped in a local minima around this cultural subset. . If you are interested in this issue, I recommend reading the paper Datasheets for Datasets and following up on the work of the author Timnit Gebru. I also recommend the work of Rachel Thomas, who has excellent blog posts and YouTube videos on ethical theory and organisational practice. . If you are interested in Data Sampling specifically, there are a lot of “stats 101” courses covering how to take samples fairly. . Ontological and Data Silos . Ethical oversights certainly extend beyond Data Science research and into applied Data Science. All organisations are prone to measurability bias, optimising towards what can be measured. What is measured is a result of our sampling, and we cannot measure the success of our sampling independently of further sampling. Systematic errors from missing onta and missing data are ignored because they can’t be measured, so sampling success is ignored. . Domain Driven Data Science is a methodology to escape ontological silos and account for ontology-driven systematic errors. We can overcome missing-data-driven systematic errors by improving modelling techniques and multi-modal/multi-domain data analysis. . Linking claims data with EMRs enables control for confounders that are missing in analyses based on claims data alone. . Real-World Evidence, Causal Inference, and Machine Learning . Data Scientists are determined to be data-driven in their decision-making. But redefining our problems around new data samples and ontologies produces different incommensurable paradigms of thought. No metric-driven decision-making can enable these paradigm shifts due to their incommensurability. There is an underlying culturally embedded art at play in our decision-making, which we must be aware of to make ethical decisions. . Fundamental Limitations of Scientific Studies . Experiments aim to determine treatment effect by mimicking observing a factual and its corresponding counterfactual. An objective experiment is impossible as if one side of the comparison is obtained; the other cannot be if everything else is kept equal. . All experimental study designs must balance ease of execution against internal and external validity. Internal validity is the measure of how objective the experiment’s results can be. An experiment has greater internal validity when it avoids introducing bias via data sampling, placebo effects, information leakage, block design strategy and axiomatic assumptions underlying the experimenter’s worldview. External validity is the measure of generalisability from your sample to your target population and generalisability beyond the target population you sampled. . Experiments often have the implicit goal of having external validity regarding generalisation to future populations. These future populations, by definition, cannot be sampled, so it is impossible to objectively assert this degree of external validity. This is the same flaw underlying any induction; past behaviour is not a perfect predictor of future outcomes. . The philosophy of science has shown that the goal of complete internal consistency is impossible and logically inconsistent, and external validity is also impossible to prove. . The lack of external validity is at the core of the scientific reproducibility crisis. . More than 70% of researchers have tried and failed to replicate another scientist’s experiments, and more than half have failed to reproduce their own experiments . 1,500 scientists lift the lid on reproducibility . An experiment requires true randomisation, which is impossible to assert. Every design is made with non-functional constraints and within the researcher’s worldview, which provides them with only the tractable choices available within their understanding. Every experiment is fundamentally a quasi-experiment. Objective truth is infinitely far away, but many results are clearly closer to the truth than others. . The philosophy of science has proved that humans are limited to always taking a leap of faith. The practice of science and experimental design is to do our best to minimise that leap. . We can achieve good enough randomness within a target population to provide as strong as possible internal validity. Given good enough internal validity, we continue to debate the best way to enable good enough external validity. Our methodological practices are not perfect and have much room for further progress. . To determine the efficacy of a treatment or intervention in healthcare, evidence is used from experiments with randomised controlled trials (RCTs) as the golden standard. It is recognised that these results are “under ideal conditions”, so observational studies are used to monitor the rollout of treatments into the “real world” to collect real-world evidence. However, several studies have shown that basing decisions on observational studies are, on average, just as effective as basing decisions on RCTs. More research is required in the meta-analyses of RCTs and observational studies to understand why this is the case in practice when it is not in theory. . Fundamental Limitations of Data Science . If a Data Scientist is interested in genuinely investigating causal relationships to make more “definitive” predictions and claims, they can use machine learning for hypothesis generation just as observational studies can be used as a precursor to experiment design. . Some researchers are looking to directly integrate machine learning with causal inference via techniques such as targeted maximum likelihood estimation. These techniques look to assert causation within a target population under axiomatic assumptions. However, this does not escape any challenges from other experimental designs or alternative approaches to causal inference. Data sampling continues to drive the result. . There is nothing magical about machine learning that protects against the usual challenges encountered in observational data analysis. In particular, just because machine learning methods are operating on big data does not protect against bias. Increasing sample size—for example, getting more and more claims data—does not correct the problem of bias if the data set is lacking in key clinical severity measures such as cancer stage in a model of breast cancer outcomes. . Real-World Evidence, Causal Inference, and Machine Learning . Studies use observational databases to gather “real-world evidence” of impact to monitor the rollout of a treatment. These studies must choose a specific database and enact their data sampling. There is no objectively correct data waiting to be analysed without data sampling first. Madigan et al. estimate that 20% to 40% of database studies can switch from a statistically significant result in one direction to a statistically significant result in the other direction, depending on the data set used for the analysis, even after controlling for common study design. . Such studies begin by selecting a particular database, a decision that published papers invariably report but do not discuss. Studies of the same issue in different databases, however, can and do generate different results, sometimes with strikingly different clinical implications. . NCBI - WWW Error Blocked Diagnostic . Many researchers deeply believe in the merits of their theory-driven model estimation, and they believe in the Bayesian methodology of their causal analysis. But, they cannot deny the increasingly evidenced power of the frequentist approaches taken by machine learning. The Bayesian methodology is often viewed as limiting us to the confines of what we already know or think we know, with the frequentist approaches viewed as theory agnostic. The belief and hope that machine learning “lets the data speak for itself”. However, this is false, and this belief can be ethically dangerous. . The philosophy of science has shown the theory-dependency of observation. Machine learning cannot be theory agnostic. Our data is theory-laden, our sampling methodologies are theory-laden, and our modelling techniques are theory-laden. We cannot escape the biases of our theories. You will always introduce bias, which could cause over or underestimates for the whole population or subgroups within it. . Effective and Ethical Data Science Study Design . To be effective and ethical in the application of data science, we must always be cognizant. The best way to explore our biases is to challenge our thinking. There are many questions a Data Scientist should consider: . What is my objective? | What kind of study am I carrying out? | What is my target population? | Is it even possible to sample from my target population? | What non-functional constraints do I have, and how will they affect the objectivity and generalisability of my results? What is my budget? What are my deadlines? How confident am I in gathering increased funding? | How do these constrain me? What questions am I able to ask? What level of rigour am I able to achieve? | . | What are the inherent biases in my study design? How can I remove, mitigate or acknowledge these? | What are the inherent biases in my data sampling methodology? How can I remove, mitigate or acknowledge these? | What features and feature values are present in my ontology? What are their measurement scales? How does this impact their distribution and my ability to represent them? | How can I discover new features and feature values I don’t even know are possible? Who could provide me with a different perspective to discover these? | . | . | What data values are present, and what relevant data may be missing? Which limitations can I overcome with better modelling techniques or broader multi-modal or multi-domain data integration? | . | What does good enough internal validity look like? How do I define this? How do others define this? What biases influence this threshold? | What methodology will I use to determine good enough external validity? What are its limitations? Are there alternatives? Can I improve this? | What underlying theories are at play that laden my data? Can I account for these? Can I mitigate their risks? | . Effective and ethical practice go hand in hand. Biases are systematic errors. “Random errors” are unaccounted for systematic errors. We can improve our Data Science practice, we can improve our data science methodology, and we can improve our assessment of methodologies. All of these contain bias, and all of these have errors. All can be accounted for and improved through rigorous thought and curiosity. .",
            "url": "http://www.robbiepalmer.com/data-science/science/philosophy/2023/02/24/effective-ethical-data-science-study-design.html",
            "relUrl": "/data-science/science/philosophy/2023/02/24/effective-ethical-data-science-study-design.html",
            "date": " • Feb 24, 2023"
        }
        
    
  
    
        ,"post1": {
            "title": "Navigating Titles in the Data & Machine Learning Market",
            "content": "Navigating the titles in the Data market used to be simple. Analysts were hired to explore Business Intelligence, with niche and relatively narrow skill-sets. With the onslaught of big data, and the ever increasing complexity of data analysis, for an ever increasing variety of use-cases, the market has became much more confusing. . This transition is similar to that of the general software world. When computer usage was limited, companies could hire an “IT guy”. Now they navigate roles such as Software Engineers of many types (front, back, full), DevOps Engineers, System Administrators, SCRUM Masters etc. . With the Data / ML market being young and immature, many of the titles are used fuzzily, or just completely incorrectly. This prevents organisations from attracting, hiring and retaining the appropriate talent to deliver their goals. It is common for titles to completely mismatch intention, leading to project failure. . Below I have outlined the most commonly used titles in this space, with the scope of their encompassing skill-sets, and how they inter-relate to each other. . Software Engineer . A software engineer builds solutions to automate, semi-automate or enrich business processes. Software engineers are responsible for collaborating with domain experts and product management to understand challenges a user faces in typical user workflows. They explore user personas and workflows and collaborate to propose solutions technology can provide. . Software engineers will have various forms of expertise to solve different parts of problems in different ways. Different software engineers may have skill-sets useful in building: . Web apps On-premise | In the cloud | . | Phone apps | Desktop apps | Embedded systems | . Within these apps / systems different software engineers may have skill-sets useful for: . User interface / experience design | Implementing the user interface and linking to back-end services | Designing cloud systems | Translating business logic into robust, consistent logic | Data storage / streaming architecture | . The title of software engineer is the most generic in this space. In the market there is a large supply of software engineers, but an even larger demand. The software engineer title is the most frequently found, due to its longer history and broader context. . Data Scientist . Just like a software engineer, a Data Scientist builds solutions to automate, semi-automate or enrich business processes, but they do so via a different methodology. A Data Scientist investigates, researches and applies knowledge of statistics and probability to solve problems. . The Data Scientist title is likely the second oldest title in this field, and can be used broadly in its application. To try to target more specific subsets Airbnb have three types of Data Scientist roles. . In practice Data Scientists have become increasingly associated with the application of Machine Learning tooling, as ML has become the go to solution for an increasing number of problems. Aka the role AirBnB calls an Algorithms Data Scientist. What Airbnb calls an Analytics Data Scientist may be more commonly found now as a Data Analyst, and an Inference Data Scientist perhaps a statistician. . Different Data Scientists have skill-sets useful for working on different types of problems with different types of data, such as: . Tabular data analysis | Image analysis | Natural Language Processing | Recommendation systems | Data in range of: Megabytes, Gigabytes, Terabytes or Petabytes | Specific domains such as BioTech, FinTech or BI | . Within these contexts different Data Scientists may have skill-sets useful for: . Data Collection / Sampling / Validation | Data Engineering / MLOps | Data Visualisation | Applying Classic ML Algorithms | Applying Deep Learning | Problem framing / Domain exploration | Technical communication Internal | External: Blogs, papers, patents etc. | . | . The power of the Data Science methodology was recognised decades ago, but the tooling and processes for wide adoption have only recently been developed. Due to this, Data Scientists often feel closer to academia than industry. . In many organisations Data Scientists are within a research department, publishing papers and creating patents rather than working within product teams. Often these roles may go under the alternative title “Research Scientist”. They often differ from academia in terms of research scope, incentive structures and career progression, while having much overlap in day-to-day activities. . As the tooling and processes have matured, Data Scientists are being increasingly hired in a product specific context. In a product context, ML tends to be applied in a very specific part of a user workflow. In this context it is important for Data Scientists to understand the user, their domain and the full workflow to decide on an appropriate problem framing for their sub-solution. . Data Engineer . A Data Engineer typically builds internal solutions to solve problems faced in the training and application of Machine Learning models. A Data Engineer can typically be viewed as a form of specialised Software Engineer, focused on Data storage, networking and the Data Science domain, given their “customer” is internal employees applying Data Science. They also tend to take on skill-sets from a DevOps Engineer role since both these roles are focused on automating internal engineering processes. They may also take on responsibilities typically covered by a Cloud Architect role in certain organisations where the data scale is large enough to require such solutions. . Different Data Engineers have experience with different technologies that are more suitable for different types of data / problems that need solved, such as: . Hadoop with HDFS for big data problems that can be divided into independent units | Apache Spark for more flexible big data problems | Docker, Docker Swarm, Kubernetes for repeatable, scalable application of programs in any language using any framework | DVC / MLFlow / Pachyderm / KubeFlow / Weights &amp; Biases as underlying infrastructure for creating ML Pipelines | GitHub Actions / Bitbucket pipelines / GoCD for CI/CD pipelines, possibly combined with ML Pipelines | Luigi for complex, long-running batch job pipelines | Relational, Document, Key-Value and / or Graph databases for different data structures | . Machine Learning Engineer . Given the broad scope of the Data Scientist title, and its historical linkage with academia; the new title Machine Learning Engineer has emerged to be more focused and targeted on a business need. . An ML Engineer typically has many traditional Software Engineering skills, combined with Data Science knowledge. Where a Data Scientist may be more focused on theory and research, an ML Engineer will be more focused on tools and application. . An ML Engineer’s role and responsibilities will overlap with those of Software Engineers, Data Scientists, Data Engineers and DevOps Engineers. ML Engineers will be focused on functional and non-functional requirements of technological solutions. Some Data Science solutions may functionally solve a problem, but not meet the desired non-functional requirements, such as cost to run, execution time, throughput, maintenance overhead etc. A good ML Engineer will take all of these into account when exploring problem and solution framing. They will typically use a combination of Software Engineering and Data Science to meet functional requirements and utilise Software Engineering / DevOps / Data Engineering tooling to meet non-functional requirements. . Different ML Engineers will have different skills that map to a subset including any of the skills of a Software Engineer, Data Scientist and Data Engineer, typically sampling from across all three. . There have been past titles used for this role that haven’t caught on across the market such as: . Algorithm Engineer | Research Engineer | .",
            "url": "http://www.robbiepalmer.com/software/data-science/management/2022/06/29/navigating-titles-in-the-ml-market.html",
            "relUrl": "/software/data-science/management/2022/06/29/navigating-titles-in-the-ml-market.html",
            "date": " • Jun 29, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Are All Experiments Actually Quasi-Experiments?",
            "content": "Randomised Controlled Trials (RCTs) are the gold standard for scientific experimentation. The theory is that a truly random process (not influenced by any external features of the environment) enables us to produce a representative sample of the population upon which to experiment. Given this sample is truly representative with no bias introduced via the sampling process, we are able to progress with further experiment design to aim for objective results regarding efficacy and causality. . RCTs are fundamentally based upon the ability to generate a truly random sampling process for your target population. This is baked into the definition of an experiment. If samples are not randomly allocated, then a study is definitionally a quasi-experiment. On investigating if experiments are possible, one of the first questions to ask is “is random sampling possible?”. Not just is it practical, and feasible for scientists executing experiments, but is it even theoretically possible within the universe? The existence of objective randomness has been debated for millennia, underpinning belief in free will or deterministic fate. . Theoretical physics, being the study of our universe at the most fundamental level, has been particularly challenged by this philosophical debate. Many physicists have tried and failed to objectively prove randomness or determinism one way or another via logic and experiment. . The model of classical physics suggests the Computational Universe Theory under which no process is truly random. Classical physics models the universe as one with fundamental laws of nature. The progression of the universe and time, is the unfolding of the application of these laws. These laws of nature are theoretically discoverable. In this model of the universe, if we know all laws of nature, we can systematically determine any outcome. Nothing would be theoretically unknowable. Therefore, nothing would be objectively random. Randomness is a subjective illusion. Whenever something appears random, it means we simply do not have the appropriate theoretical or experimental framework in place to account for the systematic laws generating the observed behaviour. . This provides the basis for the story of scientific progression. Where once the whole world appeared random and ungovernable, or under the governance of supernatural Gods; the scientific revolution brought us faith that we could master the universe via the scientific method. As our mastery has increased with scientific revolutions, we have gained greater and greater technological power, making what was previously observed as random, now explainable and therefore predictable and controllable. . The field of quantum mechanics has split the physics community on their belief in a stochastic or deterministic universe. In quantum physics, phenomena appear random and probabilistic, but also bring up numerous paradoxes. On top of these paradoxes we are also yet to rectify it with general relativity, highlighting the incompleteness of our understanding. Physicists have many quantum theories, speculating on its fundamental basis: . The Copenhagen interpretation of quantum mechanics states that the universe is intrinsically non-deterministic. Truly random and probabilistic. This theory also believes that quantum descriptions are truly objective; completely “independent of physicists’ mental arbitrariness”. The Copenhagen interpretation has strong challengers. They show the subjective need for an observer, and the need for a measuring device premised upon “non-objective” or at least unaccounted for classical mechanics. | The relational quantum mechanics interpretation claims quantum descriptions are not truly objective, but are observer-dependent. This is based on the idea underlying special relativity, of an observation being dependent upon the reference frame of the observer. | Hidden-variable theories believe that quantum mechanics is a useful, but incomplete model of the universe. Some of these theories, like Bohm’s theory, believe that a hidden deterministic principle is driving observed perceptually probabilistic phenomena. Some theories like stochastic quantum mechanics believe the hidden driver behind quantum mechanics’ expression is itself random. In stochastic quantum mechanics this driver is stochastic fluctuations in spacetime. | The many worlds interpretation of quantum mechanics states that all possible outcomes do deterministically occur, but not all within the universe we inhabit. Some claim this theory is irrelevant as it is untestable, since we cannot observe what occurs in other universes. Others claim we could make this testable versus the Copenhagen interpretation, by placing macroscopic objects in a coherent superposition and interfering with them. This experiment is currently beyond our capabilities. | The transactional interpretation of quantum mechanics builds on the Wheeler–Feynman absorber theory, stating that electromagnetic waves travel both forwards and backwards in time. This theory breaks away from stochasticism vs determinism, by instead suggesting retrocausality. | . It is clear, there is no consensus, nor logical or empirical proof as to whether true randomness is possible in the universe. Even worse, the relational interpretation challenges objectivity and the transactional interpretation challenges causality. More fundamentals of experimental design are under challenge than simply random allocation alone. . Given our current state of our knowledge of the universe we must simply take a leap of faith one direction or another. Choose randomness versus determinism, and even causality versus retrocausality. Progress in philosophy and especially epistemology has shown we can never prove determinism nor randomness. To try to do so would come under the frame of induction, which has been shown to lack a strong logical foundation. . Our approach to determining the objective status of the result of an RCT falls into the Münchhausen trilemma, as any attempt at objective proof does. Requiring we rest on a dogmatic argument, a regressive argument or a circular argument; regarding the ability to truly randomly assign samples and more. Axiomatic statements have been proven to be completely logically inconsistent and pseudo-scientific. . If we cannot even prove the objectivity of our most isolated fundamentals, we need a heaping of humility when it comes to claiming objectivity of our experiments. Our macroscopic experiments have many more interacting dynamics and all theoretical experiments are conducted within a culturally determined paradigm. . RCTs and the scientific method may not have a proven objective basis but they are undeniably useful. All models are wrong, some are useful. Humanity is fundamentally limited to working with wrong but increasingly useful models of the universe. Some of these models are based on our scientific methodology. A model for how to discover new models. We are unable to arrive at the truly objective. We cannot infinitely de-risk our actions. We must take a leap of faith to some degree. Our improving scientific methodologies continually lower what we consider acceptable levels of faith to act upon. We choose a preferred frame, but ensure we shift it towards objectivity bit by bit. . Our current status quo can be to dogmatically state that true randomness is possible under practised experimental design. If we wish to improve our scientific methodology, we must recognise that this dogmatic claim does not have a fundamentally proven basis. We must recognise the quasi-experimental nature of our current scientific methodologies. If we wish to remain open to future scientific revolutions, we must be willing to step out of our current paradigm. I believe we should recognise our fallibilism and embrace the joy of venturing into the infinite unknown of scientific methodological progression. .",
            "url": "http://www.robbiepalmer.com/philosophy/science/2022/03/28/quasi-experiments.html",
            "relUrl": "/philosophy/science/2022/03/28/quasi-experiments.html",
            "date": " • Mar 28, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "The Philosophy of Data Science",
            "content": "Data Science is the practice of extracting knowledge from data. Philosophy is the study of the fundamental nature of knowledge, reality and existence. To create Data Science methodologies, we must know what knowledge is. . The Philosophy of Data Science must look at: . What is data science, and what is it not? | What is the goal of data science? | What are the methodologies under which it is practised? Are these reliable? | . | What is its impact? Ethical considerations for society | . | . The Philosophy of Data Science overlaps with many other philosophies of science such as: . Philosophy of mathematics | Philosophy of statistics | Philosophy of technology | . The practice of Data Science also overlaps with the challenges of the domain to which it is applied. A Data Scientist working in the field of medicine needs to account for challenges from the philosophy of medicine. A Data Scientist will rely on domain experts, who may have differing philosophies, or who may bias the problem-solution via an implicit philosophical consensus. . To be a “good” Data Scientist, you must face challenges from all these fields. The philosophical practices under which you operate may be implicit or explicit. I believe surfacing implicit philosophies can drive us towards better methodologies. . Only by making the implicit explicit, can we analyse what we currently do, how we currently measure success, and how we may wish to grow going forward. . I believe a “good” Data Scientist should challenge their own philosophies with curiosity; applying many of the underlying principles of Data Science to their own psychology. This is important for effectiveness as well as ethics; understanding how to frame problems and how to determine when you have an effective solution. . My goal in this essay is to help frame a discussion around how the various schools of thought in the philosophy of science, and their progression, translates to data science practices. . TLDR: . Summary of Perspectives | Impact on Data Science Practice | Post-positivist Domain Driven Data Science | What Does This Mean For Me? | . The Evolution of Scientific Practice &amp; Philosophy . Empiricism . Before the scientific method appeared, the dominant means of determining “truth” was via authoritative anecdote. This led people to have beliefs such as “mice spontaneously generate from dirty rags”. . Sir Francis Bacon defined the scientific method, influenced by the work of Copernicus and Galileo. He was an Empiricist. He setup the principles of experimental science to be: . Repeatable | Measurable on a universal scale | Economical | Consilient | Heuristic | . Data management and analysis is at the heart of empiricism. An experiment’s procedure is stored as inspectable data. Measurements generate data. Heuristics are calculated from data. Repeatability and consilience of theory are proven through data collected via experiment. . The insights of the empiricists, enabled via their data management, drove the Scientific Revolution. (Woodruff, 2019) However, Empiricism has its limits. Taken to its extreme, it can prevent insight and action. The extreme Empiricist will claim you cannot know something to be true unless you have direct experience of it yourself. They will claim that what you cannot experience is unknowable. . This is captured by the focus on being heuristic and economical: . A heuristic technique is not guaranteed to be optimal, perfect, or rational, but is nevertheless sufficient for reaching an immediate, short-term goal or approximation. (Heuristic, n.d.) . Of course, an Empiricist cannot reproduce every experiment themselves, to provide them with direct access to all available truths. In practice they would accept they must operate on faith; their core tenant being that until they have reproduced it themselves, it stays but faith. . In accordance with the principle of Occam’s razor, many would restrict their “leaps of faith” to be as small as feasible. When relating to data, an Empiricist may only believe something can happen when that exact scenario has been recorded, thus minimising their leap of faith. . Logical Positivism . Logical Positivism was a movement striving to exalt science and drive scientific and technological progress. It overtook Empiricism as the mainstream scientific philosophy in the early twentieth century. (Logical Positivism, n.d.) . Where Empiricism is a philosophy based only on observation; Logical Positivism aims to be a bridge that can unite theory and observation. It expands the definition of true, beyond that which can be directly experienced, to include logical and mathematical truths. Its goal is to be a system by which statements can be defined and clarified. This process verifies fundamental truths or “axioms” which act as a foundation for understanding. . Scientific theories are viewed as strings of axioms strung together. The practice of science is to deduce a theory, then verify its statements and downstream consequences using empirical data. If a statement can’t be empirically verified, via a verifiability criterion, Logical Positivism views it as completely meaningless, and unscientific. If it cannot be verified, it cannot form an axiom on which a theory can be built and so no knowledge can be derived from it. . Logical Positivists build their theories from both observational and theoretical terms. Observational terms denote objects or properties that can be directly observed or measured, while theoretical terms denote objects or properties we can only infer from direct observations. This treatment of inference as a tenet of truth, is one of its key differences versus Empiricism. . To connect theory and observation, Logical Positivists create rules of correspondence to translate between them. These are themselves theories of how translation can occur. Theories of how the abstract and the concrete can be bridged. These rules of correspondence, however, cannot be empirically proven. This self-contradiction is one of the major criticisms of Logical Positivism. . Logical Positivism believes there is no limitation to human knowledge. It believes all knowledge is accessible via experience and logic combined. It believes science is the only source of knowledge. It believes all scientific disciplines can be unified into one single science, under theorems strung together by fundamentally proven axioms. . The Vienna Circle directly attempted to unify all science via the International Encyclopedia of Unified Science. . Logical Positivism has Data Science practises even more fundamentally at its core than Empiricism. It envisages knowledge as a graph of facts, both observational and theoretical; and their inter-relations. With new axioms or theories causing ripples of analysis and verification upwards and downwards respectively. . Criticisms of Logical Positivism . The flaw in Logical Positivism is that it falls into the Münchhausen trilemma. Rules of correspondence are designed to use observations as foundations to support theories. But these rules of correspondence are themselves theories, which need to be supported. . A Logical Positivist must either make the dogmatic argument, simply asserting that rules of correspondence are true without being able to defend them with empirical evidence. Or they must make the regressive argument that there are rules of correspondence to support rules of correspondence, carrying on ad infinitum. Or they must make a circular argument such as “the chosen rules of correspondence are clearly true, because they prove these theories connect to these observations” when they are explicitly designed to do this by their nature. . Given Logical Positivism’s rules of correspondence cannot be empirically verified, it in effect declares itself meaningless and unscientific. . Some of Logical Positivism’s main proponents tried to save the philosophy. Carnap tried to use Craig’s theorem and Ramsey’s theorem to avoid the dogmatic argument with regards to theoretical terms, by showing they could be reduced to observational terms. In effect, this was an attempt at a return to Empiricism. However, this did not provide the foundation Carnap hoped for. This work shows that a theory based on theoretical terms can be reduced to one based on solely observational terms, but it does not guarantee that these are finite. Which many would argue means this is not sufficiently finitary to be meaningful. . Later Carnap took a different angle to instead tackle the epistemic regress problem. He split “theoretical laws” from “empirical laws”; where empirical laws are simple generalisations from observational terms. Theoretical laws are produced via deduction, to explain the empirical laws. But this has the same issue as the rules of correspondence. Logical Positivism still fails in its core mission to connect these abstract theories to observational reality. . Induction follows deduction. . Hans Reichenbach, (founder of the Berlin Circle, promoter of Logical Empiricism, sister of the Vienna Circle’s Logical Positivism) came to abandon the idea of foundationalist reductionism. In his later work he asserts that observations only provide probabilities for object descriptions, and that these probabilities do not outline a definitive action. . The final nails in the coffin were the work of Kuhn, Quine, Hanson, and other post-positivists. They undermined the division of observation and theory. They outlined how all observations (and therefore all datasets) are inherently laden with theory. Data is inherently contextual, observations cannot be decoupled from theory, a datum cannot provide an impartial ground truth. . Logical Positivism’s downfall is linked to its meta-epistemological goals. It is a philosophy within the frame of naturalism; but it stretches to be a meta-philosophy. It claims that metaphysical statements are pointless and meaningless, because it makes the dogmatic claim that metaphysical naturalism is objective truth. . Instead of admitting that it makes a dogmatic claim to metaphysical naturalism, it claims that all metaphysics should be rejected. Ironically this is a meta-metaphysical position known as metaphysical deflationism. . Logical Positivism tries to use its own philosophy to prove the meta-philosophy on which it relies. Work in the philosophy of mathematics has shown that this is impossible. The foundational crisis of mathematics came from the assumption that mathematics has a logically consistent foundation that can be stated within mathematics itself. This was undermined by various paradoxes such as Russell’s paradox. It has also been undermined by Gödel’s second incompleteness theorem; which establishes that logical systems of arithmetic can never contain a valid proof of their own consistency. . This can be extended to show that a philosophy cannot consistently prove the meta-philosophy within which it resides. This is particularly relevant to Logical Positivism which makes this meta-philosophical claim. . Gödel’s second incompleteness theorem proves the limits of provability via formal axiomatic theories, which Logical Positivism claims can be exceeded. . Hilbert wanted to prove a logical system S was consistent, based on principles P that only made up a small part of S. But Gödel proved that the principles P could not even prove P to be consistent, let alone S. (Foundations of Mathematics, n.d.) . Critical Rationalism . Karl Popper approaches the quest for truth from the opposite direction, via his philosophy of Critical Rationalism. Where Logical Positivism aimed to establish fundamental observational axioms of truth on which to build theories; Critical Rationalism refines the theories we are capable of creating. Popper did not believe that science could be reduced to a formal logic system or method. . He viewed theory as “extra-logical”. Theories are produced by creative conjecture which is beyond the grasp of reason. Logic and empirical observation is invaluable in its role of falsification and refutation of theories. . Logically, no number of positive outcomes at the level of experimental testing can confirm a scientific theory, but a single genuine counter-instance is logically decisive. (Karl Popper, n.d.) . Popper believed empirical verification is logically impossible. You cannot deduce theory from data. Data does not imply theory. Many theories can fit a dataset. You can only ever say what is not. You can only falsify, not verify. . Logical Positivism defines what is verifiable as meaningful and what is not as meaningless. Critical Rationalism defines what is falsifiable as scientific and what is not as unscientific. But it does not view what is unscientific as meaningless. What is currently unscientific can become scientific (falsifiable) through evolving knowledge and technology. . Logical Positivists attacked the unscientific, where Popper here finds value. Popper criticised not the unscientific, but the pseudo-scientific. He saw this as acting against the scientific moral value of the quest for truth. . The pseudo-scientific are those who proclaim the unscientific as definitively true, or those who persist with a previously scientific theory that has now been falsified. The former would often define experiments that by their definition would be true using circular arguments. These experiments are designed to get cultural credence via association with the scientific method. The latter would often try to protect the failing scientific theory by adding ad-hoc clauses or reaching for increasingly speculative hypotheses. . Popper would view Logical Positivists as being pseudo-scientific, for continuing to believe the philosophical theory following proof that it is self-contradictory. Many of Logical Positivism’s main proponents did try to patch the theory as Popper described, increasingly weakening their claims. (Logical Positivism, n.d.) . Falsifying our existing theories, causes error elimination. This process of error elimination causes a selection pressure on our theories, leading to a “natural selection” driving an evolution of ideas. Popper believed in objective truth, but just as “biological fit” does not predict continued survival, neither does “theoretical fit”. Therefore, not all evolutionary steps are towards objective truth, but conjecture and refutation drive us towards more interesting problems. . Theories that better survive the process of refutation are not more true, but rather, more “fit”. (Karl Popper, n.d.) . The ideas of evolutionary fit, falsification and iterative steps towards tackling more complex environments / more interesting problems are at the core of data science, machine learning, agile practises and domain exploration. Aka at the heart of modern applied data science and machine learning engineering. . One challenge in applying critical rationalism, is hypotheses cannot be tested in isolation, they are built within an environment of theories. Data falsifies the whole package of relevant theories, making them, together with this data, incompatible. The falsifying data does not directly point to which of the theories within the environment are wrong. . Popper proposes addressing this by targeting the least general theories within the environment for replacement. E.g. When the motion of the planet Uranus was found not to match the predictions of Newton’s laws, the theory “there are seven planets in the solar system” was rejected, and not Newton’s laws themselves, leading to the discovery of Neptune. . The more general a theory, the more valuable it is. This re-introduces data into the role of targeting progression. The theories which account for the smallest amount of a knowledge graph are the most likely to be revised. . Another applied challenge is how to definitively claim a hypothesis has been falsified, as the process of generating the falsifying data may itself have contained errors. . It is not always clear that if evidence contradicts a hypothesis that this is a sign of flaws in the hypothesis rather than of flaws in the evidence. (Karl Popper, n.d.) . Popper believed the process of resolving this lay on the creative judgement of the scientists themselves, just as in the case of generating new theories / hypotheses in the first place. The need for scientists to have faith in their own results shows the need for robust data practises, with error correction protocols in place. These practises themselves are fallible hypotheses that should continually be challenged and improved. . Postpositivism is not a rejection of the scientific method, but rather a reformation of positivism to meet these critiques. It reintroduces the basic assumptions of positivism: the possibility and desirability of objective truth, and the use of experimental methodology. (Postpositivism, n.d.) . Another applied approach to Critical Rationalism may be via reverse mathematics, a program which aims to go “backwards” from theorems to axioms. . Criticisms of Critical Rationalism . Critical Rationalism falls into the Münchhausen trilemma just like Logical Positivism. This means it is an unscientific theory by its own definition. But unlike Logical Positivism, it doesn’t define this as meaningless. . Critical Rationalism makes a dogmatic argument with regards to creativity, simply asserting that it is extra-logical, something not to be understood via rational inquiry. . In his book The Beginning of Infinity, David Deutsch makes it clear that Critical Rationalism does not try to avoid epistemic regress. Rather it embraces infinite regress as an exciting frontier to explore. (Deutsch, 2012) . Critical Rationalism is not “concerned with the rational reconstruction of scientific knowledge” like Logical Positivism. It is concerned with “the real process of discovering”. Unlike Empiricism, which is hesitant to take leaps of faith, Critical Rationalism embraces creative leaps of faith as essential to scientific advancement. . Where Empiricism and Logical Positivism proclaim metaphysical naturalism, Critical Rationalism takes the more moderate view of methodological naturalism. Methodological naturalism is the view that naturalism should be assumed in one’s working methods as the current paradigm, without any further consideration of whether naturalism is true in the robust metaphysical sense. . A Critical Rationalist will view metaphysics as unscientific, but not meaningless. It does not try to solve the metaphysical criticisms outlined above, it simply premises this need for creativity to be extra-logical, as its metaphysical basis. . Post-positivists would likely take the meta-metaphysical view that Critical Rationalism’s metaphysics is problematic. The Duhem–Quine thesis outlines how this creative judgement is inherently theory laden, making unambiguous scientific falsifications just as impossible as unambiguous scientific verification. . Kuhn put forward his theory on paradigm shifts, arguing that it is not simply individual theories but whole worldviews that must occasionally shift in response to evidence. (Bird, 2018) This would suggest that empirical evidence can necessitate us updating our metaphysics, not just our physics. . If we view the path forward for knowledge advancement as a light-cone, Critical Rationalism would view empirical evidence as objects blocking the light’s path, allowing us to constrain our focus. Kuhn’s work would suggest we can rotate the direction of the light, pointing it in a different direction where these objects may no longer be relevant. . Post-Positivism . Thomas Kuhn is credited with “killing” Logical Positivism by progressing the post-positivism thesis of the theory-dependence of observation. (Bird, 2018), (Postpositivism, n.d.) Within this framing, a datum is not inherently insightful on its own, it can only be insightful in context with the intent and protocol under which it was generated. . Kuhn claims that theories, and how they relate to the data that provides evidence for or against them, are judged relative to a “paradigmatic theory”. This paradigmatic theory is the current scientific consensus; shared theoretical beliefs, values, instruments, techniques, and metaphysics. Standards, therefore, are not theory independent rules. Standards are not permanent. Paradigm shifts occur causing changes in standards. . This theory-dependence means each revisionary scientific revolution brings about an entirely new worldview or paradigm, and the old paradigms cannot be understood from within the new; they speak entirely incompatible languages. They are “incommensurable”. . Kuhn believed that there were many scientific revolutions throughout history, but these are hidden because the teaching of science is naturally done from within the new paradigm, and inherently cannot communicate the old, keeping them hidden. Kuhn gives examples of the words matter or atom, and says Aristotle’s Physics ‘wasn’t just bad Newton’, it was just different. . Both Positivists and Critical Rationalists believe that “revolutionary science” and “normal science” come about by the same means, with revolutions sought, promoted and welcomed. Kuhn claims that the process of change is more similar to religious conversion than a process of verification or falsification. Kuhn believed normal science can succeed in making progress only if there is a strong commitment by the relevant scientific community to their existing paradigm. . Because commitment to the disciplinary matrix is a pre-requisite for successful normal science, an inculcation of that commitment is a key element in scientific training and in the formation of the mind-set of a successful scientist. (Bird, 2018) . The incommensurability of paradigm shifts has huge implications for the practice of data science and data engineering. Each revolution that brings about a new worldview / paradigm invalidates the data of the old. All data that depends on the old paradigm will need to be translated to, or regenerated within, the new paradigm to be comprehensible and insightful. . In his book “The Essential Tension” (1959) Kuhn outlines the tension between the desire for innovation and the necessary conservativeness of scientists. The cost of a shift means revolutions are not sought except under extreme circumstances, when a number of particularly troublesome anomalies have arisen that can’t be accommodated under the existing paradigm. . Theories are incommensurable when they share no common measure. Thus, if paradigms are the measures of attempted puzzle-solutions, then puzzle-solutions developed in different eras of normal science will be judged by comparison to differing paradigms and so lack a common measure. (Bird, 2018) . Kuhn outlines three types of incommensurability. . Methodological incommensurability occurs when methods of comparison and evaluation change. For example, if two different machine learning models are tackling the same problem, but are evaluated against two different validation datasets; the resulting metrics are incommensurable and any attempt at comparison is meaningless. Another example is when successive product managers apply different value weightings to different metrics. What is success in one period does not equal success in another. An “objective” level of success cannot be established between these. . Methodological incommensurability can also come about when there is disagreement about what problem a team should even solve; how a customer’s problem is framed to enable proposed solutions. What even is the target? There is no common measure for different targets even when abstractly targeting the same goal. . Observational incommensurability occurs when different worldviews result in different fundamental perceptions. An example is how you could glance at your desk, and “see” your phone where it always is. Someone else could glance and “see” an external hard-drive in the same location. The lowest level of what each person can conceive of as “seeing” is their mind’s semantic interpretation of the signals the eyes received. This semantic interpretation is moulded by each person’s worldview / paradigm etting their basis of expectation. Without more data this fundamental observational divergence will be unresolvable. . Observational incommensurability is highly relevant to data collection and labelling, and a methodology is required for resolving divergences in perception. . Semantic incommensurability occurs when the same symbols or words are used to denote different meanings in different times / paradigms. If this is constrained to singular words this is manageable, but Kuhn believed in meaning holism: the claim that the meanings of terms are interrelated in such a way that changing the meaning of one term results in changes in the meanings of all related terms. An example of meaning holism causing semantic incommensurability is the transition of Newtonian to Einsteinian physics. The change in paradigm redefined a “whole conceptual web whose strands are space, time, matter, force, and so on”. . Kuhn’s views on semantic incommensurability derive from Quine’s thesis of the indeterminacy of translation. Quine outlined that when translating from one language to another there are multiple translations, none of which are uniquely correct, and that the connection between translations is impossible to recover. Kuhn believed this translation wasn’t impossible, just very difficult. . A significant scientific change will bring with it an alteration in the lexical network which in turn will lead to a re-alignment of the taxonomy of the field. The terms of the new and old taxonomies will not be inter-translatable. (Bird, 2018) . If meaning holism is true, then even minor alterations to data will have a much larger impact than most would assume. . A post-positivist may embrace pragmatism in contrast to the previous philosophies’ naturalism. This rejects the idea that the function of thought is to describe, represent, or mirror reality. Words and thoughts are tools and instruments for prediction, problem solving, and action. This is similar to Critical Rationalism, but doesn’t make the claim that scientific advancement leads us to objective truth. ”Objectivity” in this meta-philosophical framing is about the direction in which we evolve to solve problems related to our observed experience. Post-positivists emphasise the need to examine your own values and beliefs, and correct for their biases when possible. This includes your “choice of measures, populations, questions, and definitions”. . While positivists believe that research is or can be value-free or value-neutral, postpositivists take the position that bias is undesired but inevitable, and therefore the investigator must work to detect and try to correct it. Postpositivists work to understand how their axiology (i.e. values and beliefs) may have influenced their research. (Postpositivism, n.d.) . Criticisms of Post-Positivism . Kuhn claims “revisionary change” is a necessary condition for “revolutionary change”. Aka a paradigm must be overthrown as in the case of Einstein’s impact. . Some critics point to what they view as revolutionary change occurring without paradigm shifts, such as the discovery of the structure of DNA leading to a “revolution” in molecular biology. . My interpretation of Kuhn’s work is that this disagreement comes from a difference in semantics applied to the word “revolutionary”. I don’t believe that Kuhn is saying huge progress can’t be made within a paradigm; but is using the word revolution to identify revisionary changes. I believe he wished to highlight how many revisionary changes have occurred and how they are likely to occur again. . I believe acknowledgement of revisionary change is essential for data practises. . Computationalism . Post-positivists and Critical Rationalists have both moved beyond the logical reductionism of Logical Positivism. Computationalists, however, believe most have retained baggage about how to tackle scientific problems. . If we do not believe in the supernatural. If we believe the universe is underpinned by some laws of nature that we could theoretically discover. If we believe in causality and the forward progression of time (rather than retro-causality). Then one must believe that the progression of the universe and time, is the unfolding of the application of these laws. . The existence of the universe comes about via the computation of these laws of nature. These computations come from some “beginning”, which this theory does not try to address. This is the Computational Universe Theory. This connects also with the Computational Theory of Mind, which models our mind in the same way. . Like Logical Positivists, a Computationalist, believes there is a consistent first-order theory underlying the universe. But they do not claim that this is tractable / accessible, like the Logical Positivists did. . Any consistent first-order theory (such as that which underlies the universe) gives the logic to construct a model of the world. A consistent worldview is emergent from a consistent first-order theory. There is an iterative process of completing a theory that produces the model. Every step adds axioms to the model and the model can be checked for consistency amongst the axioms. . Any computational step can prove that the theory is inconsistent, but its consistency will forever remain unprovable as the steps are infinite. This is outlined in Critical Rationalism, proven in Godel’s incompleteness theorem, and shown in the Turing Proof regarding the Halting problem. We can only assume consistent theories if we assume the existence of a true infinity. This connects with the Platonist view that mathematical objects are as “real” as any physical object, as they’re required to “exist” in reality to underpin our physical reality. Some, like Stephen Wolfram believe this foreshadows “A New Kind of Science”. . Wolfram emphasises how we have proven the universality of numerous sets of rules found in many systems. The study of Computer Science has shown that the universe is in effect a Universal Turing Machine containing numerous Turing Complete systems (computer instruction sets, programming languages, cellular automata). These universal simple rules generate arbitrary levels of complexity, which implies that there is a single inherent scale of complexity; nothing is theoretically out of reach to a system given appropriate resources. . He says all theoretical breakthroughs are about reducing the amount of computation required to predict how a particular system will behave. . Most of the time the idea is to derive a mathematical formula that allows one to determine what the outcome of the evolution of the system will be without explicitly having to trace its steps. (Wolfram, 2002, 737) . Many scientists do believe that all the laws of nature can be reduced to beautiful, elegant expressions of mathematics. Einstein showed this with his groundbreaking E = mc². Such a simple expression, with implications that rocked our society. But many systems elude simplification via mathematical formulae. A famous example being the Three-body problem. . On first-sight this can appear to be a relatively simplistic problem, but it has been proven that there is no general closed-form solution (solvable by a finite number of standard mathematical operations). There is a function that is an analytic solution but it cannot escape the need for heavy computation. Greater computation in this case provides greater precision. To produce the level of precision required for astronomical observations, the computations would involve at least 108,000,000 terms. This shows how astronomers are already facing the limitations of our ability to reduce complexity. . Wolfram claims that the fact that “There are many common systems for which no traditional mathematical formulas have ever been found that readily describe their overall behaviour” is a consequence of computational irreducibility. He believes our current approach to scientific advancement relies on the implicit assumption that we can find the outcome of an evolution of a system with much less effort than modelling said system. Computationalists believe it’s impossible to find a “Theory of Everything” that reduces computation of the underlying system as: . If meaningful general predictions are to be possible it must at some level be the case that the system making the predictions be able to outrun the system it is trying to predict. Nothing can systematically outrun the universal system… any system that could would in effect have to be able to outrun itself. (Wolfram, 2002, 737) . We cannot build a model within the universe that can outrun the universe. If the universe is full of computational irreducibility, then we cannot fully model the universe from within the universe. This connects with the issues of Logical Positivism trying to prove its own meta-philosophy from within its own philosophical framework. Which was disproven by Gödel’s second incompleteness theorem. . Wolfram believes this reducibility theory has dominated due to the conservatism inherent within a paradigm, as outlined by Kuhn. . I suspect the only reason that [theoretical scientist’s] failure has not been more obvious in the past is that theoretical science has typically tended to define its domain specifically in order to avoid phenomena that do not happen to be simple enough to be computationally reducible. (Wolfram, 2002, 737) . This implies that traditional scientific progression has not been inherently driven towards objective truth, or even simply guided by an evolution towards better fit problems as claimed by Popper. But rather, scientific progression has been guided by which theories happen to be both theoretically accessible via creativity, but also computationally reducible in a given moment. . Even when we have the theoretical insight to model a problem, we have been unable to progress a computationally irreducible theory. We can only progress it via more effective computational methods. Those who believe science has progressed via new tooling more so than new theories, will be partial to this idea. Many in the field of Machine Learning believe Deep Learning has become the leading solution for many problems for a similar reason, due to GPUs becoming increasingly prevalent, making their approach to solving a problem tractable. . Computational irreducibility means certain problems can only be modelled rather than reduced. If we wish to tackle these then we must focus on better modelling, and accept we can only gain greater precision ad infinitum, not certainty. Wolfram believes the number of computationally reducible processes is finite. If true, this will cause the end of scientific methodology as we know it. . Computationalists believe the future of scientific advancement lies in the field of Data Science, supported by Computer Science. . Summary of Perspectives . Empiricism: Data is the only truth. . Logical Positivism: Data combined with logic leads to the only truths. . Critical Rationalism: Unscientific creative thought is essential for the progress of science. This is constrained and falsified by data. This applies a selective pressure to drive the evolution of better theories to solve more interesting problems, which generally moves towards objective truth. . Post-positivism: Paradigm shifts occur to better solve problems within an environment. Inhabiting a paradigm is essentially binary. Normal progress requires full commitment to an existing paradigm. Revolutionary progress requires a full shift to a new paradigm. The meaning of data is inherently dependent upon the paradigm in which it is generated and viewed. Comparing data between paradigms is effectively impossible. . Computationalism: All theoretical advancements are simply reductions in required computation, which is impossible for certain types of problems. The future of scientific advance hinges on computational models and their emergent behaviour. . Impact on Data Science Practice . I propose that the goal of data science is to drive scientific progression in an evolution towards better solutions, fit for better problems, with the purpose of creating a better life. . We do not nurture data for data’s sake. We take actions, we observe their outcomes, and based on our observations we act again. We live our lives in feedback loops. Data derived from observations only provides value when it shapes our actions. . Often we gather data without knowing its future impact, especially in the theoretical realm. It can often be hard to credit data with impact as it may be subtle, or comprehension of its impact can be incommensurable, such as on our methodology for making decisions rather than directly on any single decision itself. But whether gathered to tackle known unknowns or unknown unknowns, its purpose is always to tackle something via action. . This is to say; data is useless if it cannot lead to action. If it cannot be used to update our theories which drive our practises. According to Kuhn’s theories, data can only lead to action when contextualised within an appropriate problem space at an appropriate level of abstraction. There are infinite problem framings, so it is impossible to derive an appropriate context from an arbitrary datum; which is why we work the other way round. . We start with creative hypotheses and use data to drive them. We have problem framings we care about tackling, and we seek appropriate data that adds value within its context. This is why Data Management Practises are so vitally important to an organisation’s success. . A Logical Positivism philosophy underlies the assumptions of a Data Warehouse. The belief that “all scientific disciplines can be unified into one single science” is similar to the belief in a Data Warehouse that “all data can be unified into a single schema”. . The process of applying a verification function to determine what is true, false or meaningless, is equivalent to a Data Engineer applying a methodology for determining what data entries are accommodated into the warehouse as true or false, or what are completely disregarded and ignored due to being “meaningless”. . A Data Lake abandons the Logical Positivism philosophy and allows diverging data schemas to exist alongside each other. From the Critical Rationalist perspective, this enables the exposure of problems to resolve, when data contradicts other data. Theories are conjectured to resolve these problems and all data gathered together in the lake is used to constrain creative thought, as any datum can falsify a theory. The organic growth of the lake drives the evolution of theories to solve more and more complex problems. . A Data Lake accepts fallibility and redirects focus from forcing coherence now, to iteratively searching for more interesting problems of incoherence and solutions to these. . Post-positivism underlies a Data Mesh. Kuhnian thought suggests that Data Lakes will inevitably turn into Data Swamps due to paradigm shifts. In a Data Swamp old paradigms are intermingled with new, and the combination of incommensurable language makes interpretation impossible. . Data Meshes isolate domains which operate within a paradigm, with a team whose focus is within that paradigm or to use the equivalent term from Domain Driven Design, its Bounded Context. When a Dataset’s paradigm shifts, a Data Mesh approach enables downstream Datasets to continue interfacing with old data in the old paradigm consciously (if API versioning is enabled), or translate their ingestion of the dataset to work with the new paradigm. . The downstream team can determine whether or not this new paradigm change has consequences for their own domain model. Therefore, paradigm shifts are safely (though not necessarily easily) accommodated and propagated throughout the whole data system. . Post-positivist Domain Driven Data Science . Kuhn’s claims have a huge impact on data science practice. They imply that all Data Engineering, Software Engineering, Software Architecture, User Stories, Machine Learning Engineering and Data Science practises must be taken into account relative to standards for assessing data. The results of a Machine Learning model will not be independent of these, it is derived from within this paradigm. Being conscious of these factors leads us to Domain Driven Data Science or paradigm conscious design. . Eric Evans’ book Domain Driven Design (DDD) outlines a post-positivist approach to software engineering. Many of its principles apply to the practises of Data Science, and Data Engineering. (Evans, 2004) The belief that a shared commitment to a paradigm is required to enable normal incremental progress is equivalent to having a strong Bounded Context. DDD practitioners use context maps to identify different language sub-paradigms, enabling models of different domains to be evolved and implemented independently. . DDD practises are methodologies for encouraging the “normal progression” of science within a sub-paradigm or domain. All domains still exist within an overarching paradigm, as shown by the need for a ubiquitous language to handle domain relations. So DDD doesn’t escape the challenges raised by post-positivism, but it does aid engineers / scientists in addressing them to continue to make “normal progress”. . Any divergence from a domain results in leaky contexts. This corrupts the domain, and often neighbouring domains with incommensurable language. This makes them increasingly unworkable until a paradigm shift / rewrite is carried out. . Incommensurable paradigms is also the basis for the common phrase used by software developers of Consistency Over Correctness. When a new paradigm is unveiled, it cannot be fully adopted without huge investment. The existing software exists within the context of the old paradigm. Whatever concepts the new paradigm invalidates will need re-written, along with any interrelations. . Even if you believe a new paradigm is more “correct” a partial update of your system will lead to a mixture of incommensurable terms split between multiple paradigms, which is worse than a system contained solely within the old paradigm. This is often the problem at the heart of “legacy systems”. Software systems can become a mixture of incommensurable terms, similar to how a Data Lake becomes a Data Swamp. . What Does This Mean For Me? . Everyone within your organisation will have a philosophy that underlies how they relate to data and data practices. The philosophies held by those in your organisation (especially those with authority and those working closely with data) shapes your culture, the perspectives your organisation is able to consider, and therefore the actions it is able to take. . Understanding these perspectives, and their natural consequences / solutions will help you inter-relate with others with diverging worldviews, or detect when group-think is occurring to highlight potential flaws in your decision making. . When making decisions we always have to compress information into a form that is comprehensible within our brains, and to a form comprehensible between brains when working in a team. To do this, we make assumptions and work with overly simplified, lossy worldviews. This isn’t inherently bad, it is necessary to enable action. . The more advanced philosophies may be less lossy and better represent the world, but they are different tools for different jobs. Just as a Data Warehouse is the best solution to some problems (to balance your functional and nonfunctional requirements), Logical Positivism may be the best team philosophy to operate under. . I simply encourage you to recognise the philosophy under which you operate, so that if it ever stops serving your needs, you can detect this, and adapt as appropriate. . Even beyond the inherent flaws / limitations of a philosophy itself; the most prominent philosophy of science in your organisation will have downstream cultural effects. These derive not directly from fundamental truth claims of the philosophy, but from how they get held in practice. Implicit ideas lead to explicit practises which influence your ability to execute. . Your technical implementations are manifestations of the solutions you produce in your mind. The closer these match, the easier it is to translate theory into practice. . Make sure your tooling and methodology matches the philosophy under which your decisions are made. . References . Bird, A. (2018, October 31). Thomas Kuhn. Stanford Encyclopedia of Philosophy. Retrieved February 14, 2022, from https://plato.stanford.edu/entries/thomas-kuhn/ . Deutsch, D. (2012). The Beginning of Infinity: Explanations that Transform the World. Penguin Books. . Evans, E. J. (2004). Domain-Driven Design. Addison-Wesley. . Foundations of mathematics. (n.d.). Wikipedia. Retrieved February 14, 2022, from https://en.wikipedia.org/wiki/Foundations_of_mathematics#Foundational_crisis . Heuristic. (n.d.). Wikipedia. Retrieved February 14, 2022, from https://en.wikipedia.org/wiki/Heuristic . Karl Popper. (n.d.). New World Encyclopedia. Retrieved February 14, 2022, from https://www.newworldencyclopedia.org/entry/Karl_Popper . Logical positivism. (n.d.). Wikipedia. Retrieved February 14, 2022, from https://en.wikipedia.org/wiki/Logical_positivism#Popper . Logical positivism. (n.d.). New World Encyclopedia. Retrieved February 14, 2022, from https://www.newworldencyclopedia.org/entry/Logical_positivism . Postpositivism. (n.d.). Wikipedia. Retrieved February 14, 2022, from https://en.wikipedia.org/wiki/Postpositivism#History . Wolfram, S. (2002). A New Kind of Science. Wolfram Media. https://www.wolframscience.com/nks/p737–computational-irreducibility/ . Woodruff, T. K. (2019). Ingredients of Scientific Success: People, Ideas, Tools, and Teams. Endocrinology, 160(6), 1409–1410. https://academic.oup.com/endo/article/160/6/1409/5489425 .",
            "url": "http://www.robbiepalmer.com/philosophy/data-science/2022/03/02/the-philosophy-of-data-science.html",
            "relUrl": "/philosophy/data-science/2022/03/02/the-philosophy-of-data-science.html",
            "date": " • Mar 2, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "Post-modern Management",
            "content": "Market conditions are forcing us to make a break with managerial practices of the past. . Labour’s efficiency growth has been dominating productivity since the industrial revolution. Previously, investments in physical capital (such as roads) drove productivity. . The industrial revolution marked the moment when there was humongous growth of output per worker, without the corresponding investment in physical capital. . This has been driven by an era of industrial capitalism, characterised by the division of labour to enable specialisation and the routinisation of work tasks. . As the non-creative parts of tasks become disentangled from the creative parts, they can increasingly be made more efficient by applying standardised processes, semi-automation and eventually total automation. . These efficiency gains apply a selection pressure, driving further splitting of tasks into creative and non-creative parts. . In the modern era following the industrial revolution the creativity in business was done at the strategic management level of existing businesses or by entrepreneurs for new businesses. . An entrepreneur would discover a re-arrangement of tasks that enabled these efficiency protocols, and setup an organisation to exploit this discovery to undercut their competitors in the market, or to enable the birth of a new market that was previously economically inaccessible. E.g. Henry Ford inspired by the “dis-assembly” line in a slaughterhouse, setup an assembly line for automobile construction. . Managers hire non-creative labour to plug the holes in the process where creative and non-creative work are still coupled, or where an economically viable, totally automated solution for some non-creative work has not yet been found. . In the past, creatives were ignored if they had no power / authority. They head no means by which to enact their creations. Only those with financial backing and social status could bring their creations to life in broad markets, often employing labourers. . As technology has advanced, we continue to discover ever more efficient methodologies and build ever more powerful tools. This has led to reorganisations occurring and non-creative work vanishing at an astonishing rate. . A positive feedback loop has been kicked off. New efficiencies, lead to more efficently discovering new efficiencies. . Discovering and implementing efficiencies enables us to discover and implement meta-efficiencies. . As the discovery of efficiency protocols accelerates, non-creative labour is disappearing and new creative work is being created. . That being, the creative work of generating these efficiency protocols. . Labour is increasingly shifting to be the process of creatively disrupting existing labour. . Strategic management continues to be creative, but the growth in creativity is happening with the labourers; and management is struggling to adapt to this change. . To creatively generate efficiency protocols, a holistic overview of the whole system is required. Specialisation and tight focus hinders creativity. Generalism and holism drives creativity. . This contrasts sharply with the methodology of the division of labour that drove modern industrial capitalism. . The goal of [creatives] is not to execute. Rather, the goal is to learn and develop profound new business capabilities. . The managerial practices of the modern era are becoming increasingly defunct in our contemporary post-modern or metamodern society. . Organisations that employ modernist managerial practices will stifle the business innovation and employee growth required to compete in an increasingly creative market. . Adam Smith himself criticizes the division of labor, suggesting that it leads to the dulling of talent — that workers become ignorant and insular as their roles are confined to a few repetitive tasks. By contrast, generalist roles provide all the things that drive job satisfaction: autonomy, mastery, and purpose. . The future is owned by organisations that can employ new managerial practices designed to nurture creatives. . Organisations that can nurture holism within individuals. . Organisations that can have all as one, and one as all. . Organisations that can drive meta-efficiencies. .",
            "url": "http://www.robbiepalmer.com/management/economics/philosophy/2022/02/07/post-modern-management.html",
            "relUrl": "/management/economics/philosophy/2022/02/07/post-modern-management.html",
            "date": " • Feb 7, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "Just Right Engineering",
            "content": "Everyone in tech loves to hate on the over-engineers or the under-engineers. . Over-engineers say: . Software engineering is declining! | No one understands underlying hardware or algorithms anymore! | No one truly understands this system, look at their dependencies! They depend on millions of lines of code! | I’ll write huge style guides, set up technical committees, gatekeep all code changes and re-write the whole system from scratch. Then everything will be great and I’ll finally sleep at night. | . Under-engineers say: . Git sucks! Code reviews suck! OOP sucks! Architectural design sucks! | Everyone is wasting their time, just throw something together and push it out! | I love hacking, I’ll spend all evening and weekends doing it, I’m always happy to jump on top of my bugs to squash them :^) | . Tech draws in people who are super passionate about theory or super passionate about application. . The middle ground is left frustratingly empty. . The art of engineering is in choosing the right pattern at the right time for the right problem. . Both over-engineers and under-engineers are guilty of ignoring this art. . The meta-problem here is how does one solve a problem. . Both parties treat the meta-problem as solved. . In reality, they’re both doing what they find most fun, and aren’t even trying to solve the meta-problem. . They’re applying a one-size fits all approach, projecting out the world they want to exist; but the reality is: . Life is nuanced . All good software engineering tools, techniques and paradigms are there to enable you to handle the appropriate level of complexity. . An over-engineer will completely fail to launch a simple project and so in that context won’t deliver anything of value. . But their mindset may be what is needed for say; a clinical grade medical device running on embedded devices. . An under-engineer will completely fail to improve a vast complex system and so in that context won’t deliver anything of value. . But their mindset may be what is needed for say; building a new fan site for a buzzing hot topic, where time is of the essence, and flare is more valued by users than robustness. . So all engineers can be “just right engineers” by moving to an appropriate environment. But in practice very few will slot into a perfect role for their default make-up. . The engineers I respect most are those who adapt to their environment. . Just right engineers say: . This approach would have huge value, but requires huge investment. The return on investment is too low, so we will not use it at this point of time | This approach is high risk, high reward. There are no low risk, high rewards alternatives; so lets commit. | We are currently using an approach that would enable future functionality we would love to build, but we don’t know if / when we will build it; and for now it is adding huge overhead to our process holding us back. We will drop it for now. | We have not invested in tooling, and now that we are scaling it is massively holding us back; time to commit to building these enablers | . The key being applying the art of engineering . Avoiding doing something too soon or too late . Following the nuances of life . Moving fast enough that you take action to shake out unknown unknowns . Moving slow enough to observe the consequences of your actions, and spot opportunity to enhance productivity . All software engineering should be a balancing act between committing to deliver value now and increasing your future ability to deliver value .",
            "url": "http://www.robbiepalmer.com/software/2021/12/10/just-right-engineering.html",
            "relUrl": "/software/2021/12/10/just-right-engineering.html",
            "date": " • Dec 10, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "How to Build Wealth",
            "content": "Money shapes our lives. . We spend 80,000 hours working to earn money. . We spend many more spending it or simply worrying about it. . Everyone needs financial literacy to get through life. . Yet we are not taught the basics of how to handle money. . There are three simple, (not easy, but simple) steps to build wealth: . Spend less than you earn | Avoid debt | Invest the rest | Wealth . Wealth gives you freedom to take risks. . If you can take risks, you can: . Change jobs | Demand better pay from your boss | Pursue work that makes you happy | Take time between jobs to organise your life | . Taking these risks will improve your life and increase your income. . More Wealth means more Freedom . More Freedom means more Chances . More Chances means more Income . More Income means more Wealth . More Chances makes a Happier Life . Spend Less Than You Earn . If you spend more than you earn, you&#39;re spiralling towards bankruptcy. . If you spend exactly what you earn, then you&#39;ll be a slave to your job forever. . If you spend less than you earn, you can build wealth. . There are two ways to spend less than you earn: . Earn more (without spending more) | Spend less (without earning less) | &quot;A penny saved is better than a penny earned&quot; - A financially literate grandmother . Cutting spending is more effective and is often easier than increasing earnings. . For the median UK earner, because of tax: . Earning an extra £1 per year, increases their wealth by 84p per year | Saving an extra £1 per year, increases their wealth by £1 per year | . Spending less also makes what you have go further. . In terms of financial freedom, halving your spending is the same as doubling your earnings. . I recommend Mr Money Moustache&#39;s Blog if you&#39;re interesting in spending less. . He is challenging, funny and extremely effective at cutting spending. . Avoid Debt . It is pointless saving or investing if you have debt. . Debt is incredibly powerful. . It will wipe out the value of any savings or investments you have. . It will prevent you from ever gaining any wealth. . Debt uses the same principles that powers investments, but instead of working for you it works against you. . The average interest for UK credit card debt is 19% . The average interest for UK savings accounts is 1.2% . The expected returns for investing in the market is ~7% . The difference in growth is exceptional. . The graph below shows what happens to £1000 worth of debt, savings and investment, left untouched for 20 years. . Once you have no debt, start saving. . To avoid debt it is important to have an emergency cash fund. . As a rule of thumb: have enough cash to cover a few month&#39;s expenditure. . If you need to spend money, always use savings. Never take on debt. . If you have no savings and have to take on debt, pay it off ASAP. . Use your surplus cash to relentlessly pay off debt. . Start with the highest interest loans. . Student loans and mortgages are special cases . In the UK student loans are more similar to a graduate tax than debt . Mortgages are leveraged investments . Invest The Rest . The government has many schemes to encourage saving and investing. . In my experience it often does the opposite. . To safely invest, you need to understand: . Types of accounts with their: Eligibility rules | Deposit rules | Withdrawal rules | Tax implications | Various charges | . | Types of investments with their: Volatility | Diversification | Liquidity | Potential for growth | . | . You then need to choose your investment platforms and sift through platform specific funds etc. . People either make a jab with partial knowledge or give up trying. . Below is my overview of these points and my broad-brush advice. See the disclaimer. . Types of Investment Accounts . When it comes to types of accounts it makes sense to: . Maximise your employer pension contribution (this requires a minimum investment from you) | Max out a Lifetime ISA, if you&#39;re eligible and will buy a house or stay invested until you&#39;re 60 | Open a Stocks and Shares ISA and invest up to £16k-20k per year | If you still have spare cash (lucky you!) invest in a standard investment account | When choosing which accounts to use, you must trade-off between potential growth and flexibility. . Pension . Most people are unable to withdraw from their pension account until they are at least 55 years old. This makes it the least flexible investment account. . Pensions enable the highest returns because: . Your pension gets invested before income tax | By law, your employer must pay you more if you save in your pension | The money you would have paid as income tax earns interest while invested. You will get to keep this interest. . You will have to pay income tax when withdrawing from your pension. But if you have no or little other income when you withdraw, you will get to take advantage of your tax free personal allowance. . If you save 5% of your pre-tax income in your pension, then you employer must pay you an additional 3% of your salary, which goes straight into your pension. . This is the minimum, many employers are more generous and will give you an even bigger pay rise. . The graph below shows the difference in your return on investment if you invest: . In your pension with employer contributions | In your pension without employer contributions (e.g. after maxing out your employer&#39;s offer) | In an ISA with your extra take home pay (what would have went into your pension) | Assumptions This assumes all investments have an identical return of 7% per year. The values used for the plot are based on the median UK salary of £15.38 per hour and the median UK working hours of 37 hours per week. Other incomes within the same tax bracket should have equivalent differences. This assumes the earner has an employer who contributes the minimum required by law to their employee&#39;s pension. This means a contribution equal to the value of 3% of the employee&#39;s pre-tax salary, excluding the first £6250 of earnings. As required to trigger the minimum employer contribution, the employee is assumed to contribute 5% of their pre-tax salary, excluding the first £6250 of earnings. The calculations take into account income tax and national insurance, assuming the employee&#39;s salary is their only income. If you earn the UK median salary, and you decide to save nothing in your pension you will have an extra £934 per year in your pocket. . If you take advantage of the 5% employee, 3% employer incentive you will get £1868 per year in your pension. . Lifetime ISA . A LISA is a government scheme to encourage investment and first time home buying. . It is more accessible than a pension but also has lots of rules around eligibility, deposits and withdrawals. . You cannot open a LISA if you&#39;re younger than 18 or older than 39. . You should definitely invest in a Lifetime ISA if you plan to buy your first home (which you should only do for lifestyle reasons). . It also makes sense if you have maxed out your employer pension contribution and plan to keep this money invested until you&#39;re 60. . You pay into a LISA with your after tax income. . You can contribute up to £4k per year until you&#39;re 50 years old. (But you must open an account before you&#39;re 40). . The government adds 25% to your investment. If you&#39;re a basic rate tax payer, this is equivalent to a refund on your income tax. . In this case it will perform the same as an employee only pension contribution. . Explanation If you have £100 before income tax and pay the basic rate of 20% you will receive £80. If you invest this £80 in a LISA the government will add 25%, bringing your account balance to £100. You can withdraw from this account fee free if: . You&#39;re over 60 | You&#39;re a first time home buyer using this money towards a deposit | . You can withdraw money at any time for any other reason, but you will pay a fee. . The fee is usually equivalent to losing ~6% of your investment. . Explanation If you invest £80 in a LISA the government will add 25%, bringing your account balance to £100. If you withdraw money without meeting the criteria, you will have to pay a 25% charge, leaving you with £75. This is 93.75% of the original £80. Investing in a LISA and withdrawing fee free is much better than investing in a normal ISA. . Investing in a normal ISA is marginally better than a LISA with fees. . Investing in a LISA and withdrawing with fees is much better than not investing. . So don&#39;t let the fees put you off too much! . ISA . You can invest up to £20k per year in an ISA. . Any investments in a Lifetime ISA count towards this total, so if you max out your LISA, you can invest up to £16k in a normal ISA. . Standard Investment Account . In a standard investment account you may have to pay taxes on the interest you earn. . You may pay tax every year based on the increase in value. . You may have to pay capital gains tax when you withdraw your cash. . ISAs are preferable so that taxes aren&#39;t a concern. . Types of Investment . You can invest in: . Company stock - you own a percentage of a company | Bonds - you own the debt of governments or companies | Property | Commodities - you own natural resources like gold, oil or grain | . Each of these has its own: . Volatility | Liquidity | Potential for growth | Minimum investment | . An investment is volatile if its price changes dramatically over short periods of time. . An investment is liquid if you can easily turn it into cash. . Company Stock . Volatily: High | Liquidity: High | Potential for growth: High | Minimum investment: Low | . Bonds . Volatily: Low | Liquidity: Medium | Potential for growth: Low | Minimum investment: Low | . Property . Volatily: Low | Liquidity: Very Low | Potential for growth: Medium | Minimum investment: Very High | . Commodities . Volatily: Low | Liquidity: Medium | Potential for growth: Low | Minimum investment: Medium | . Single Investments vs Investment Funds . You could invest in a single company, bond, house or commodity, but . Investing in a single asset is very high risk. . Diversification is how you protect yourself against unpredictable changes that affect single types of assets. . Don&#39;t put all your eggs in one basket. . You can actively pick many individual investments that sound promising to you. . Don&#39;t do this. To have a chance you would need to dedicate your career to it. . Even then, over a 10+ year period you will do worse than if you just spread your investment across the market. . There is a reason Warren Buffet is famous. He is one of the only people who has successfully beat the market. Almost everyone else lost, while working hard for the privilege. . Don&#39;t try to beat the market. Join it. . The solution is investment funds. . They spread your money across many individual assets. . You can choose funds with different scopes for different types of assets; such as different sectors of the economy, different countries, different lengths of investment etc. . If you want to maximise diversity you can spread your investment across all publicly traded companies and bonds. . Active funds have paid professionals regularly changing what they invest your money in. . Passive index funds spread your investment proportionally across your investment class, representing the full market. . Passive index funds beat active funds and have much lower fees. . They should be your investing bread and butter. . Passive vs Active In any given year 80% of actively managed funds do worse than their index. A Vanguard study showed that over a 15 year timespan 45% of actively managed funds failed, another 37% did worse than their index, with only 18% outperforming it. Over a 30 year timespan 99.4% of actively managed funds failed or underperformed their index. Financial Independence . When you work you give up your time in exchange for money. . When you save and invest, you give up what you could buy now in exchange for financial security. . As shown earlier, the power of compound interest is what makes this deal attractive. . Every bit of wealth gains you more autonomy, and if you save and invest enough, you can become completely financially independent. . This means you have enough wealth that the interest it earns covers your cost of living. . It means you can no longer be forced to work out of necessity. . It means you can focus on what&#39;s important to you. Whether that&#39;s your family, your hobbies or even continuing a career, but on your own terms. . This is true &quot;F*** You&quot; money. . The rule of thumb for calculating FU money, is to multiply your cost of living by 25. . Explanation The authors of the &quot;Trinity Study&quot; investigated what percentage of their assets a retiree could withdraw every year for 30 years, without becoming bankrupt. Looking at the data for 30 years retirements spanning the years 1925-1995 they found this value fluctuated between 4% and 8.5%. If your wealth equals 25 times your cost of living, then withdrawing 4% of your wealth will cover your expenditure. Note: If you landed on a lucky year and could safely withdraw 8.5% of your wealth, then you only need 11.76 times your expenditure, not 25 times, but you wouldn&#39;t know this until the years had passed. Lets take an example of Jill. Jill earns the UK average of £15.38 per hour and works the UK average of 37 hours per week. . She has been good at controlling her expenditure, and spends what would be her take home pay if she earned minimum wage. . This means savings of ~£10k per year and cost of living of ~£14.6k a year, for a savings rate of ~40%. . The average working life in the EU is 36 years. . If Jill didn&#39;t invest she could only reach FU money as she retired. . But thanks to investing Jill has many choices. . She could carry on working the same job and retire with £1.6 million instead of £370k | She could enjoy retirement a whole 18 years earlier than planned | She could work for only 10 years then switch to part time work | . Her investments give her opportunities to control her life. . How To Build Wealth . Spend less than you earn | Avoid debt | Invest the rest | Investment guidelines: . Invest in passive index funds | For highest growth choose stocks | For highest diversification choose a global fund | To balance growth and liquidity fill your accounts in this order: Pension | LISA | ISA | Standard Investment Account | . | . If you get even part way through this list you&#39;re well on your way to growing your: . Wealth | Freedom | Chances | Income | Happiness | . Inequality . I believe the lack of financial education is a huge driver of inequality in our society. . The wealthy are taught about money; through family and friends. . The wealthless are left to hash it out on their own. . People are driven to buy a single house through the folklore that surrounds them. . And the opportunity cost is huge. . People spend years uninvested saving up the huge deposit required; even though this is less effective, riskier and less accessible than investing in an index fund. . The steps for building wealth highlight why inequality is accelerating: . Spending less than you earn is easiest for those on high incomes | Avoiding debt is easiest for the wealthy | Effectively investing the rest requires financial knowledge | . But with the right knowledge, it is possible for anyone to do all three. . Disclaimer . I am not a financial advisor. Please assess your own financial situation and carry out your own research / seek professional financial advice as required. . Though keep in mind that financial advisors are unlikely to have your best interests at heart. .",
            "url": "http://www.robbiepalmer.com/personal-finance/investing/2020/09/27/how-to-build-wealth.html",
            "relUrl": "/personal-finance/investing/2020/09/27/how-to-build-wealth.html",
            "date": " • Sep 27, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "Why You Shouldn't Buy a House",
            "content": "Buying a house is the worst decision I nearly made. . I nearly did it because it is what people expect me to do. . It seems sensible too. . You’re paying for your landlord’s retirement. . Being a landlord is profitable. . They’re obviously profitable at your expense. . Your parents and grandparents tell you their homes are worth 5-10x what they paid. . They’re horrified by your rent. . Their mortgage repayments are a fraction of this. . All of this is true. . If you buy a house you will save hundreds of thousands of pounds over your lifetime, versus doing nothing. . You don’t have to do nothing . If you’re considering buying a house for financial reasons, there are alternatives that are: . More effective AND | Less risky AND | Easier (including more accessible) | I knew there were lifestyle reasons to not buy a house, but I thought the financial gain was worth the sacrifice. . I was not aware of these alternatives. . Researching the alternatives changed my perspective of housing and changed my mind. . It might change yours too . Housing Folklore . When I wanted to buy a house, I missed that I was looking for an investment. . I did not understand that houses are just like any other investment. . There’s a lot of folk-lore and spin surrounding housing in the UK and USA. . People commonly say rent is “throwing away” your money, which they don’t say about other spending. . People talk about about climbing the “property ladder”. . Politicians talk about generations who get “left behind” because they couldn’t get on that first rung. . No one talks about other types of investment “ladders”. . No one points out that it is the lack of general savings and investments (and other systematic factors) that holds back social mobility. . There is nothing specific about houses. . Placing a deposit on a house is an alternative to investing that deposit. . Placing a deposit on a house is the same as investing that deposit. . Placing a deposit on a house is less effective, riskier, harder and restricts your life more than investing that deposit. . It isn’t only home-owners and landlords that grow their wealth. . I think the reason we miss this is that buying a house feels familiar and comfortable. . If I bought a house then wanted to move but keep my investment; becoming a landlord would also feel familiar and comfortable. . Cash and houses. They’re part of normal life. . I didn’t understand that holding cash is an investment (a terrible one at that). . I knew nothing of bonds or REITs. . Stocks are what crazy gung-ho risk takers gamble on with their millions right? . To me, these were intimidating ideas, not accessible to someone like me. . None of my friends or family know much about investing. . Their only investments are their houses and their pensions. . Pensions are black box magic. . Many view them as a special savings account the government tops up. . Houses are long term homes. . They happen to have a side effect of building equity. . I didn’t know that we all* own stocks, bonds and REITs through our pensions already. . A stock is partial ownership of a company. You are entitled to part of their profits. . A bond is ownership of someone else’s debt. You are entitled to full repayment of this loan with interest. You can sell ownership of this loan before it is fully repaid. . A REIT is a Real Estate Investment Trust. Through this trust you have partial ownership of real estate. This is usually commerical property but may include residential property. . When you are younger, your pension will mostly be a diversified set of stocks. These are the best asset for growing your wealth. . As you get older your pension shifts to bonds. . These are less volatile than stocks. They are better at maintaining any existing wealth. . This is important as you approach retirement having added money throughout your career. . Finally your pension shifts to cash so you can withdraw and spend it. . Thanks to Index Funds, we can all benefit from this process without knowing much anything about it. . A normal person with very limited financial knowledge can put their money in an index fund and study after study shows they’ll beat Wall Street traders over a 10+ year period. . I want to show how buying assets in an index fund is more effective, less risky AND easier than buying a house. . Easier . Investing in an index fund (once you know what they are and how to choose one) is as easy as opening a savings account. . For example: . Register with Vanguard online | Transfer money into your account (current minimum lump sum investment is £500) | Invest it into a LifeStrategy fund choosing the balance between stocks and bonds based on how long you will keep your money invested | . Investing in a house is famously one of the most stressful and difficult things you will ever do. . Ranked more stressful than having a baby . You need to: . Save a large sum of cash (orders of magnitudes larger than £500) | Get pre-approved for a mortgage via a mortgage broker | Optionally find a real estate agent | House hunt | Submit an offer &amp; haggle / compete with other bidders | Organise professional home inspection and appraisal | Final paperwork, pay closing costs, property tax, transaction fees, get insurance… | Move house | . This process takes months and can be further stalled by: . Struggling to find a suitable house | Mortgage application rejection | Losing out on a bid (including getting Gazumped after your offer was accepted) | Bad results from a professional home inspection | Sellers pulling out of the deal | Broken property chain | Down-valuation by lender | . Common reasons for mortgage application rejection: . You recently changed jobs | You’re not registered to vote | Administrative errors on your credit file | You don’t match the lenders target demographic | Your income is too low (counted after tax and deductions even if you could reduce deductions such as your pension contribution) | You’re self employed or a contractor | You recently applied for a credit card | You took a payday loan in the last 6 years | Your deposit is too small | . Less Risky . Investing in an index fund is less risky than investing in a house. . Diversification . Diversification is the key to managing risk. . “Don’t Put All Your Eggs in One Basket” - A wise grandmother . A house could not be less diverse. . It is in a single country, county/state, city/town, neighbourhood, street… . The value of your house will be affected by anything from global events to something a single neighbour does. . When you buy a house you’re also buying a single type of house (apartment, semi-detached, detached), with a certain style (open-plan, large windows…). . The prices for these will fluctuate based on fashion. . Index funds automatically diversify your investment. . This Global Stocks Index Fund invests in over 6700 companies spanning more than 42 countries. . This Global Bonds Index Fund spreads over 4900 debtors which include governments, companies, agencies etc. . This Global REIT Index Fund invests in many properties in more than 10 countries. . You cannot be any more diversified than that. . Everyone knows it’s risky to buy stocks in a single company. . Everyone ignores that it’s risky to invest in a single house. . Liquidity . Saying an investment is liquid or illiquid is a fancy way of saying it’s easy or hard to turn it into cash. . You need liquidity to deal with unexpected emergencies. . You never know when something bad will happen that requires a lot of cash. . You may be out of a job for a long time. . You may suddenly need to support a dependent. . All your belongings may spontaneously combust and need replaced. . A house is incredibly illiquid. . You cannot turn the money you invested into your house into cash as needed. . To access this money as cash you have to sell your house. . This takes months and involves a massive life upheaval. . More Effective . The stock market has historically grown at a higher rate than house prices. . The trick to take advantage of this is to set up a standing order to invest regularly and ignore scare-mongering financial news stories. . Leave it alone, forget about it, feel no stress and you’re set to go. . Sustaining the returns from a house requires lots of effort and stress. . To simply stop your house from decreasing in value you must carry out repairs, maintenance, redecoration and general home improvements. . This additional investment means that house prices do not reflect the whole story, and the return on investment for home ownership is lower than house prices suggest. . House prices don’t take into account additional costs required for maintenance. . Stock prices don’t take into account additional returns on investment you receive through dividends. . Dividends are your share of the company’s profits. . But what about recessions? . The Great Depression started with the worst stock market crash. . If you were extraordinarily unlucky and invested only at the peak before the crash, your investment would have recovered in 7.5 years. . If your luck was even slightly better, investing before and after the peak (especially during the trough), you would have your money back much much quicker. . That was the worst stock market crash we have ever had. . House prices are also affected by recessions. . The huge decrease in house prices caused the Great Recession of 2007/2008. . People over-valued houses just as they often over value companies, and the bubble burst when they realised the prices had got out of hand. . If you’re willing to keep your money invested in a house for a long time, then you should be willing to do the same with stocks to ride out these bumps. . Lifestyle . So buying a house is one of the most stressful things you can do, it is ridiculously difficult; it is a risky investment; and it performs worse than alternatives. . So why might you buy a house? . For lifestyle reasons: . You enjoy DIY, and your landlord prevents you from getting your hands dirty on home projects (it is possible to find landlords who will allow this) | You want to live in the same location for a very long period of time, setting down roots, being part of the community, building a sentimental attachment to your home (still possible while renting, but there is a risk the landlord will evict you) | You want to live in a specific location / in a specific style of house and cannot find something that meets this criteria on the rental market (a problem I’m currently facing) | You have found a house that is undervalued to such a degree it makes up for the financial downsides and lifestyle restrictions (you must know something no one else does or be incredibly lucky) | You want a pet, cannot find somewhere to rent that allows pets and are willing to make huge sacrifices to have one (my girlfriend) | You have a history of terrible landlords and desperately need to take back control | . Buying a house does not make sense if: . You don’t want to maintain a house | You want the flexibility to move locations, such as to other towns/cities/countries (or away from bad neighbourhoods) | You want to adjust the size of your house as needed | You want to get upgrades to appliances and decor with limited disruption (assuming moving to a newly refurbished house is less disruption than refurbishing one you live in) | You want to (relatively) easily build wealth | . Further Reading . The first time I questioned my decision to buy a house was when I read Why your house is a terrible investment by JL Collins. . That article also sparked my interest in learning about investments. . The same author has written a Stock Series of articles, discussing investments (primarily stocks but also bonds and REITs) which was so popular he used it as a basis for his book The Simple Path to Wealth. . JL Collins is popular in the Financial Independence Retire Early (FIRE) movement. . Reading about this led me to discover other interesting figures in the world of personal finance such as Mr Money Moustache. . The leading figures in this community have interesting ideas that challenge the norm. . If you believe buying a house will be a more effective investment, and you’re not put off by the difficulty, the risk and the lifestyle impacts, I would recommend running the numbers. . Renting Is Throwing Money Away … Right? runs through the complicated process of modelling your expected returns vs the alternatives. The conclusion is as with everything, it depends. Namely on: price to rent ratio, time invested, assumptions about inflation, expected costs, how fast rents increase etc. etc. . JL Collins also has a run the numbers with links to many other articles that aim to do the same. . At the time of writing interest rates in the UK are at 0.1%. I assumed this means it is a great time to buy a house, as mortgages could hardly be cheaper. . Ten Reasons It’s A Terrible Time To Buy An Expensive House made me realise that of course it’s not that simple! . The author states that it’s better to buy with a high interest rate. . The theory goes that a low interest rate causes house prices to rise. Prices will fall when interest rates rise, which we assume will happen eventually given how ridiculously low they currently are. . When house prices fall, you lose your equity but not your debt. . Markets aren’t so simple, who knew :man_shrugging: . Disclaimer . I am not a financial advisor. Please assess your own financial situation and carry out your own research / seek professional financial advice as required. . I simply hope to provoke your thoughts :) .",
            "url": "http://www.robbiepalmer.com/personal-finance/investing/housing/2020/07/25/why-you-should-not-buy-a-house.html",
            "relUrl": "/personal-finance/investing/housing/2020/07/25/why-you-should-not-buy-a-house.html",
            "date": " • Jul 25, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hi, my name is Robbie. . I plan to sporadically write about my interests, which include: . Personal finance | Data science | Software engineering | Economics | Bioinformatics | Leadership | Product development | Philosophy | People/places/food | . Many of these tie in with my current career path, others with my personal life, and the rest are included out of pure curiosity. . Machine Learning for Healthcare . The recent advances in machine learning will have a huge impact on drug discovery, cancer diagnostics and research. As with all technological leaps forward, it will require an overhaul in the way of working for all those in the field. . I’ve spent the past few years working on digital and computational pathology, and hope to help make that transition happen as fast and as effectively as possible. . My main focus is on applying computer vision techniques to images of digitised biopsies. This can free up the valuable time of clinicians and researchers by automating tedious tasks; but what will be really transformative are the new insights that are not possible without automation. . Finance . My interest in personal finance has came from planning to buy a house and changing my mind thanks to discovering the Financial Independence community or FIRE movement. . Nerdisms . An interest in politics, led to an interest in economics, which led to an interest in philosophy. Philosophy spiked my interest in how to think, which recently led to Epistemology (knowledge about knowledge). . These are super-meta subjects I find it fun to think about. I have limited knowledge about them, but I hope exploring them and writing about them can refine my thoughts. . Contact Me . If you have similar interests, want to chat or give feedback, you can leave a comment on the relevant post, or DM me on Twitter. .",
          "url": "http://www.robbiepalmer.com/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "http://www.robbiepalmer.com/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}